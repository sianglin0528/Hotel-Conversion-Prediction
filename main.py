# main.py (merged & tidy)
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report
import warnings
warnings.filterwarnings("ignore")

# ---------- 0) è·¯å¾‘ ----------
ROOT = Path(__file__).resolve().parent
TRAIN_PATH = ROOT / "train.csv"
TEST_PATH  = ROOT / "test.csv"
OUT_DIR = ROOT / "artifacts"
OUT_DIR.mkdir(exist_ok=True)

# ---------- 1) è®€è³‡æ–™ ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)
print("ğŸ‘‰ è®€æª” OK")
print("  - train å½¢ç‹€ï¼š", train.shape)
print("  - test  å½¢ç‹€ï¼š", test.shape)

# ---------- 1.1) æ¸…æ´—å‰é è¦½ï¼ˆå­˜æª” + çµ‚ç«¯é¡¯ç¤ºï¼‰ ----------
print("\nğŸ” ã€æ¸…æ´—å‰ï¼ˆtrainï¼‰é è¦½ã€‘")
print(train.head(5).to_string(index=False))
print("\nğŸ” ã€æ¸…æ´—å‰ï¼ˆtestï¼‰é è¦½ã€‘")
print(test.head(5).to_string(index=False))
(train.head(20)).to_csv(OUT_DIR / "preview_train_before.csv", index=False)
(test.head(20)).to_csv(OUT_DIR / "preview_test_before.csv", index=False)
print(f"ğŸ—‚ å·²è¼¸å‡ºé è¦½ï¼š{OUT_DIR/'preview_train_before.csv'}, {OUT_DIR/'preview_test_before.csv'}")

# ---------- 2) ç¼ºå¤±å€¼çµ±è¨ˆï¼ˆè¡¨æ ¼ç‰ˆï¼‰ ----------
def missing_table(df: pd.DataFrame, name: str):
    mc = df.isnull().sum()
    mp = (mc / len(df) * 100).round(3)
    tbl = pd.DataFrame({"ç¼ºå¤±æ•¸é‡": mc, "ç¼ºå¤±æ¯”ä¾‹(%)": mp})
    tbl = tbl[tbl["ç¼ºå¤±æ•¸é‡"] > 0].sort_values("ç¼ºå¤±æ•¸é‡", ascending=False)
    print(f"\nğŸ“Š ç¼ºå¤±å€¼çµ±è¨ˆè¡¨ï¼ˆ{name}ï¼‰")
    if tbl.empty:
        print("  - ç„¡ç¼ºå¤±å€¼")
    else:
        print(tbl)
    print(f"  - ç¼ºå¤±å€¼ç¸½æ•¸ï¼š{int(mc.sum())}")
    print(f"  - æœ‰ç¼ºå¤±çš„åˆ—æ•¸ï¼š{df.isnull().any(axis=1).sum()}")

missing_table(train, "train")
missing_table(test,  "test")



# ---------- 3) é¡åˆ¥æ¬„ä½åˆ†å¸ƒï¼ˆå¿«é€Ÿæƒä¸€çœ¼ï¼Œå¯ç•™å¯è¨»è§£ï¼‰ ----------
categorical_cols = ["Month", "OperatingSystems", "Browser", "Region",
                    "TrafficType", "VisitorType", "Weekend"]
print("\nğŸ“¦ é¡åˆ¥æ¬„ä½åˆ†å¸ƒï¼ˆtrainï¼Œå‰ 10ï¼‰ï¼š")
for col in categorical_cols:
    if col in train.columns:
        print(f"\n- {col}")
        print(train[col].value_counts(dropna=False).head(10))

# ---------- 4) ç¼ºå¤±å€¼è™•ç† ----------
print("\nğŸ§¹ æ¸…ç†å‰ trainï¼š", train.shape)
train_clean = train.dropna().copy()  # ä½ çš„ç­–ç•¥ï¼šç¼ºå°‘å¾ˆå°‘å°±ä¸Ÿåˆ—
print("ğŸ§¹ æ¸…ç†å¾Œ trainï¼š", train_clean.shape)

# testï¼šé€šå¸¸ä¸èƒ½ä¸Ÿåˆ— â†’ è£œå€¼ï¼ˆå®ˆå‚™æ€§ï¼‰
test_clean = test.copy()
num_cols_test = test_clean.select_dtypes(include=["number"]).columns.tolist()
obj_cols_test = [c for c in test_clean.columns if c not in num_cols_test]
for c in num_cols_test:
    if test_clean[c].isnull().any():
        test_clean[c] = test_clean[c].fillna(test_clean[c].median())
for c in obj_cols_test:
    if test_clean[c].isnull().any():
        mode = test_clean[c].mode(dropna=True)
        test_clean[c] = test_clean[c].fillna(mode.iloc[0] if not mode.empty else "Unknown")

# ---------- 4.x) çµ±ä¸€ ID ----------
# ä¿ç•™åŸå§‹ IDï¼Œå»ºç«‹æ–°çš„é€£è™Ÿ IDï¼ˆ1..Nï¼‰
test_clean = test_clean.copy()
test_clean["OriginalID"] = test_clean["ID"]
test_clean["ID"] = range(1, len(test_clean) + 1)

std_test_path = OUT_DIR / "test_standardized.csv"
test_clean.to_csv(std_test_path, index=False)
print(f"\nğŸ†” å·²å°‡ test.ID æ¨™æº–åŒ–ç‚º 1..Nï¼Œä¸¦è¼¸å‡ºï¼š{std_test_path}")
print(test_clean[["ID", "OriginalID"]].head(10).to_string(index=False))


# ---------- 4.1) æ¸…æ´—å¾Œé è¦½ + å·®ç•°æ‘˜è¦ ----------
print("\nğŸ” ã€æ¸…æ´—å¾Œï¼ˆtrain_cleanï¼‰é è¦½ã€‘")
print(train_clean.head(5).to_string(index=False))
print("\nğŸ” ã€æ¸…æ´—å¾Œï¼ˆtest_cleanï¼‰é è¦½ã€‘")
print(test_clean.head(5).to_string(index=False))
missing_table(train_clean, "trainï¼ˆæ¸…æ´—å¾Œï¼‰")
missing_table(test_clean,  "testï¼ˆè£œå€¼å¾Œï¼‰")
rows_dropped = len(train) - len(train_clean)
print(f"\nğŸ§® æ¸…æ´—æˆæœï¼štrain å…±ç§»é™¤ {rows_dropped} åˆ—ï¼ˆ{len(train)} âœ {len(train_clean)}ï¼‰")

# è¼¸å‡ºæ¸…ç†å¾Œè³‡æ–™ï¼ˆæ–¹ä¾¿é‡ç¾ & å°ç…§ï¼‰
(train_clean.head(20)).to_csv(OUT_DIR / "preview_train_after.csv", index=False)
(test_clean.head(20)).to_csv(OUT_DIR / "preview_test_after.csv", index=False)
train_clean.to_csv(OUT_DIR / "train_clean.csv", index=False)
test_clean.to_csv(OUT_DIR / "test_clean.csv", index=False)
print(f"ğŸ—‚ å·²è¼¸å‡ºé è¦½ï¼š{OUT_DIR/'preview_train_after.csv'}, {OUT_DIR/'preview_test_after.csv'}")

# ---------- 5) ç‰¹å¾µèˆ‡ç›®æ¨™ ----------
TARGET = "Revenue"
ID_COL = "ID"
assert TARGET in train_clean.columns, "train æ‡‰åŒ…å« Revenue æ¬„ä½"
assert ID_COL in train_clean.columns and ID_COL in test_clean.columns, "train/test æ‡‰åŒ…å« ID æ¬„ä½"

feature_cols = [c for c in train_clean.columns if c not in [ID_COL, TARGET]]
X = train_clean[feature_cols].copy()
y = train_clean[TARGET].copy()

# è½‰ 0/1ï¼ˆè‹¥å·²æ˜¯ 0/1 ä¸æœƒæ”¹è®Šï¼‰
if y.dtype == "O":
    y = y.astype(str).str.lower().map({"true":1,"yes":1,"1":1,"false":0,"no":0,"0":0}).astype(int)

X_test = test_clean[feature_cols].copy()
test_ids = test_clean[ID_COL].copy()

# ---------- 6) OneHotEncoder ç‰ˆæœ¬è‡ªå‹•ç›¸å®¹ + å‰è™•ç† ----------
def make_ohe():
    # sklearn >= 1.2 ç”¨ sparse_outputï¼›æ›´èˆŠç‰ˆæœ¬ç”¨ sparse
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=True)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=True)

num_cols  = X.select_dtypes(include=["number"]).columns.tolist()
cat_cols  = [c for c in feature_cols if c not in num_cols]

preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(with_mean=False), num_cols),
        ("cat", make_ohe(), cat_cols),
    ]
)

# ---------- 7) åˆ‡é©—è­‰é›† ----------
X_tr, X_va, y_tr, y_va = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ---------- 8) æ¨¡å‹ 1ï¼šLogistic Regressionï¼ˆbaselineï¼‰ ----------
log_reg = Pipeline(steps=[
    ("prep", preprocess),
    ("model", LogisticRegression(max_iter=1000))
])
log_reg.fit(X_tr, y_tr)
va_pred_lr = log_reg.predict_proba(X_va)[:, 1]
auc_lr = roc_auc_score(y_va, va_pred_lr)
print(f"\nğŸ§ª Logistic Regression ROC-AUCï¼š{auc_lr:.4f}")
print("\nğŸ“„ LR Validation reportï¼ˆthreshold=0.5ï¼‰")
print(classification_report(y_va, (va_pred_lr >= 0.5).astype(int)))

# retrain å…¨è³‡æ–™ + é æ¸¬ testï¼ˆå…©ä½å°æ•¸ï¼‰
log_reg.fit(X, y)
prob_test_lr = log_reg.predict_proba(X_test)[:, 1]
sub_lr = pd.DataFrame({
    "ID": test_clean["ID"].values,        # â† æ–°çš„é€£è™Ÿ ID
    "Revenue": prob_test_lr.round(2)
})
sub_lr.to_csv(OUT_DIR / "submission_logreg.csv", index=False)
print(f"ğŸ“¦ å·²è¼¸å‡ºï¼š{OUT_DIR/'submission_logreg.csv'}")


# ---------- 9) æ¨¡å‹ 2ï¼šRandom Forestï¼ˆåŠ åˆ†ï¼‰ ----------
rf = Pipeline(steps=[
    ("prep", preprocess),
    ("model", RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        n_jobs=-1
    ))
])
rf.fit(X_tr, y_tr)
va_pred_rf = rf.predict_proba(X_va)[:, 1]
auc_rf = roc_auc_score(y_va, va_pred_rf)
print(f"\nğŸŒ² RandomForest ROC-AUCï¼š{auc_rf:.4f}")
print("\nğŸ“„ RF Validation reportï¼ˆthreshold=0.5ï¼‰")
print(classification_report(y_va, (va_pred_rf >= 0.5).astype(int)))

# retrain å…¨è³‡æ–™ + é æ¸¬ testï¼ˆå…©ä½å°æ•¸ï¼‰
rf.fit(X, y)
prob_test_rf = rf.predict_proba(X_test)[:, 1]
sub_rf = pd.DataFrame({"ID": test_ids, "Revenue": prob_test_rf.round(2)})
sub_rf.to_csv(OUT_DIR / "submission_rf.csv", index=False)
print(f"ğŸ“¦ å·²è¼¸å‡ºï¼š{OUT_DIR/'submission_rf.csv'}")

# ---------- 10) å°çµ ----------
print("\nâœ… å…©æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒï¼ˆROC-AUC è¶Šé«˜è¶Šå¥½ï¼‰")
print(pd.DataFrame({
    "model": ["LogisticRegression", "RandomForest"],
    "val_roc_auc": [auc_lr, auc_rf]
}).sort_values("val_roc_auc", ascending=False).to_string(index=False))
print("\nğŸ¯ ä»»å‹™å®Œæˆï¼šè«‹åˆ° artifacts/ å¤¾çœ‹å…©å€‹ submission æª”ï¼ˆå·²å››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œå…©ä½ï¼‰ï¼Œä»¥åŠå‰/å¾Œé è¦½èˆ‡æ¸…æ´—å¾Œè³‡æ–™ã€‚")


# ===== 11) Feature Importance å¯è¦–åŒ–ï¼ˆéš¨æ©Ÿæ£®æ— + é‚è¼¯å›æ­¸ï¼‰=====
import numpy as np
import matplotlib.pyplot as plt

def _clean_feat_names(prep, feature_cols):
    """
    å¾ ColumnTransformer å–å‡º OneHot å±•é–‹å¾Œçš„æ¬„ä½åç¨±ï¼Œä¸¦å»æ‰ 'num__' / 'cat__' å‰ç¶´
    """
    try:
        names = prep.get_feature_names_out(feature_cols)
    except TypeError:
        names = prep.get_feature_names_out()
    return [n.split("__", 1)[1] if "__" in n else n for n in names]

def plot_top_importances(pipeline, feature_cols, out_path, top_n=15, title="Top Feature Importance"):
    """
    æ”¯æ´æ¨¹æ¨¡å‹çš„ feature_importances_ï¼Œæˆ–ç·šæ€§æ¨¡å‹çš„ coef_ï¼ˆå–çµ•å°å€¼ï¼‰ã€‚
    æœƒåŒæ™‚è¼¸å‡º PNG åœ–èˆ‡ CSV æ˜ç´°ã€‚
    """
    prep = pipeline.named_steps["prep"]
    model = pipeline.named_steps["model"]

    feat_names = _clean_feat_names(prep, feature_cols)

    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
    elif hasattr(model, "coef_"):
        importances = np.abs(model.coef_.ravel())
        title = title.replace("Feature Importance", "Top Coefficients (|coef|)")
    else:
        raise ValueError("æ­¤æ¨¡å‹æ²’æœ‰ feature_importances_ æˆ– coef_ å±¬æ€§")

    fi = pd.DataFrame({"feature": feat_names, "importance": importances})
    fi_sorted = fi.sort_values("importance", ascending=False)

    # å­˜æˆ CSVï¼ˆå®Œæ•´è¡¨ï¼‰
    csv_path = out_path.with_suffix(".csv")
    fi_sorted.to_csv(csv_path, index=False)

    # å–å‰ top_n ç¹ªåœ–ï¼ˆæ°´å¹³é•·æ¢ã€ç”±å°åˆ°å¤§ç•«èµ·æ¯”è¼ƒå¥½çœ‹ï¼‰
    top = fi_sorted.head(top_n).iloc[::-1]
    plt.figure(figsize=(9, 6))
    plt.barh(top["feature"], top["importance"])
    plt.title(title)
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()

    print(f"ğŸ“ˆ å·²è¼¸å‡ºåœ–æª”ï¼š{out_path}")
    print(f"ğŸ“„ å·²è¼¸å‡ºæ˜ç´°ï¼š{csv_path}")

# é‡å°ã€Œå…¨è³‡æ–™é‡è¨“å¾Œã€çš„å…©å€‹æ¨¡å‹ç”¢å‡ºåœ–
plot_top_importances(
    rf, feature_cols,
    OUT_DIR / "feature_importance_rf.png",
    top_n=15,
    title="Top Feature Importance (RandomForest)"
)

plot_top_importances(
    log_reg, feature_cols,
    OUT_DIR / "feature_importance_lr.png",
    top_n=15,
    title="Top Feature Importance (Logistic Regression)"
)
