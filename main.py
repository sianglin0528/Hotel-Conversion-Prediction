# main.py (merged & tidy)
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report
import warnings
warnings.filterwarnings("ignore")

# ---------- 0) è·¯å¾‘ ----------
ROOT = Path(__file__).resolve().parent
TRAIN_PATH = ROOT / "train.csv"
TEST_PATH  = ROOT / "test.csv"
OUT_DIR = ROOT / "artifacts"
OUT_DIR.mkdir(exist_ok=True)

# ---------- 1) è®€è³‡æ–™ ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)
print(" è®€æª” OK")
print("  - train å½¢ç‹€ï¼š", train.shape)
print("  - test  å½¢ç‹€ï¼š", test.shape)

# ---------- 1.1) æ¸…æ´—å‰é è¦½ï¼ˆå­˜æª” + çµ‚ç«¯é¡¯ç¤ºï¼‰ ----------
print("\n ã€æ¸…æ´—å‰ï¼ˆtrainï¼‰é è¦½ã€‘")
print(train.head(5).to_string(index=False))
print("\n ã€æ¸…æ´—å‰ï¼ˆtestï¼‰é è¦½ã€‘")
print(test.head(5).to_string(index=False))
(train.head(20)).to_csv(OUT_DIR / "preview_train_before.csv", index=False)
(test.head(20)).to_csv(OUT_DIR / "preview_test_before.csv", index=False)
print(f" å·²è¼¸å‡ºé è¦½ï¼š{OUT_DIR/'preview_train_before.csv'}, {OUT_DIR/'preview_test_before.csv'}")

# ---------- 2) ç¼ºå¤±å€¼çµ±è¨ˆï¼ˆè¡¨æ ¼ç‰ˆï¼‰ ----------
def missing_table(df: pd.DataFrame, name: str):
    mc = df.isnull().sum()
    mp = (mc / len(df) * 100).round(3)
    tbl = pd.DataFrame({"ç¼ºå¤±æ•¸é‡": mc, "ç¼ºå¤±æ¯”ä¾‹(%)": mp})
    tbl = tbl[tbl["ç¼ºå¤±æ•¸é‡"] > 0].sort_values("ç¼ºå¤±æ•¸é‡", ascending=False)
    print(f"\n ç¼ºå¤±å€¼çµ±è¨ˆè¡¨ï¼ˆ{name}ï¼‰")
    if tbl.empty:
        print("  - ç„¡ç¼ºå¤±å€¼")
    else:
        print(tbl)
    print(f"  - ç¼ºå¤±å€¼ç¸½æ•¸ï¼š{int(mc.sum())}")
    print(f"  - æœ‰ç¼ºå¤±çš„åˆ—æ•¸ï¼š{df.isnull().any(axis=1).sum()}")

missing_table(train, "train")
missing_table(test,  "test")



# ---------- 3) é¡žåˆ¥æ¬„ä½åˆ†å¸ƒï¼ˆå¿«é€ŸæŽƒä¸€çœ¼ï¼Œå¯ç•™å¯è¨»è§£ï¼‰ ----------
categorical_cols = ["Month", "OperatingSystems", "Browser", "Region",
                    "TrafficType", "VisitorType", "Weekend"]
print("\n é¡žåˆ¥æ¬„ä½åˆ†å¸ƒï¼ˆtrainï¼Œå‰ 10ï¼‰ï¼š")
for col in categorical_cols:
    if col in train.columns:
        print(f"\n- {col}")
        print(train[col].value_counts(dropna=False).head(10))

# ---------- 4) ç¼ºå¤±å€¼è™•ç† ----------
print("\nðŸ§¹ æ¸…ç†å‰ trainï¼š", train.shape)
train_clean = train.dropna().copy()  # ä½ çš„ç­–ç•¥ï¼šç¼ºå°‘å¾ˆå°‘å°±ä¸Ÿåˆ—
print(" æ¸…ç†å¾Œ trainï¼š", train_clean.shape)

# testï¼šé€šå¸¸ä¸èƒ½ä¸Ÿåˆ— â†’ è£œå€¼ï¼ˆå®ˆå‚™æ€§ï¼‰
test_clean = test.copy()
num_cols_test = test_clean.select_dtypes(include=["number"]).columns.tolist()
obj_cols_test = [c for c in test_clean.columns if c not in num_cols_test]
for c in num_cols_test:
    if test_clean[c].isnull().any():
        test_clean[c] = test_clean[c].fillna(test_clean[c].median())
for c in obj_cols_test:
    if test_clean[c].isnull().any():
        mode = test_clean[c].mode(dropna=True)
        test_clean[c] = test_clean[c].fillna(mode.iloc[0] if not mode.empty else "Unknown")

# ---------- 4.x) çµ±ä¸€ ID ----------
# ä¿ç•™åŽŸå§‹ IDï¼Œå»ºç«‹æ–°çš„é€£è™Ÿ IDï¼ˆ1..Nï¼‰
test_clean = test_clean.copy()
test_clean["OriginalID"] = test_clean["ID"]
test_clean["ID"] = range(1, len(test_clean) + 1)

std_test_path = OUT_DIR / "test_standardized.csv"
test_clean.to_csv(std_test_path, index=False)
print(f"\n å·²å°‡ test.ID æ¨™æº–åŒ–ç‚º 1..Nï¼Œä¸¦è¼¸å‡ºï¼š{std_test_path}")
print(test_clean[["ID", "OriginalID"]].head(10).to_string(index=False))


# ---------- 4.1) æ¸…æ´—å¾Œé è¦½ + å·®ç•°æ‘˜è¦ ----------
print("\n ã€æ¸…æ´—å¾Œï¼ˆtrain_cleanï¼‰é è¦½ã€‘")
print(train_clean.head(5).to_string(index=False))
print("\n ã€æ¸…æ´—å¾Œï¼ˆtest_cleanï¼‰é è¦½ã€‘")
print(test_clean.head(5).to_string(index=False))
missing_table(train_clean, "trainï¼ˆæ¸…æ´—å¾Œï¼‰")
missing_table(test_clean,  "testï¼ˆè£œå€¼å¾Œï¼‰")
rows_dropped = len(train) - len(train_clean)
print(f"\n æ¸…æ´—æˆæžœï¼štrain å…±ç§»é™¤ {rows_dropped} åˆ—ï¼ˆ{len(train)} âžœ {len(train_clean)}ï¼‰")

# è¼¸å‡ºæ¸…ç†å¾Œè³‡æ–™ï¼ˆæ–¹ä¾¿é‡ç¾ & å°ç…§ï¼‰
(train_clean.head(20)).to_csv(OUT_DIR / "preview_train_after.csv", index=False)
(test_clean.head(20)).to_csv(OUT_DIR / "preview_test_after.csv", index=False)
train_clean.to_csv(OUT_DIR / "train_clean.csv", index=False)
test_clean.to_csv(OUT_DIR / "test_clean.csv", index=False)
print(f" å·²è¼¸å‡ºé è¦½ï¼š{OUT_DIR/'preview_train_after.csv'}, {OUT_DIR/'preview_test_after.csv'}")

# ---------- 5) ç‰¹å¾µèˆ‡ç›®æ¨™ ----------
TARGET = "Revenue"
ID_COL = "ID"
assert TARGET in train_clean.columns, "train æ‡‰åŒ…å« Revenue æ¬„ä½"
assert ID_COL in train_clean.columns and ID_COL in test_clean.columns, "train/test æ‡‰åŒ…å« ID æ¬„ä½"

feature_cols = [c for c in train_clean.columns if c not in [ID_COL, TARGET]]
X = train_clean[feature_cols].copy()
y = train_clean[TARGET].copy()

# è½‰ 0/1ï¼ˆè‹¥å·²æ˜¯ 0/1 ä¸æœƒæ”¹è®Šï¼‰
if y.dtype == "O":
    y = y.astype(str).str.lower().map({"true":1,"yes":1,"1":1,"false":0,"no":0,"0":0}).astype(int)

X_test = test_clean[feature_cols].copy()
test_ids = test_clean[ID_COL].copy()

# ---------- 6) OneHotEncoder ç‰ˆæœ¬è‡ªå‹•ç›¸å®¹ + å‰è™•ç† ----------
def make_ohe():
    # sklearn >= 1.2 ç”¨ sparse_outputï¼›æ›´èˆŠç‰ˆæœ¬ç”¨ sparse
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=True)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=True)

num_cols  = X.select_dtypes(include=["number"]).columns.tolist()
cat_cols  = [c for c in feature_cols if c not in num_cols]

preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(with_mean=False), num_cols),
        ("cat", make_ohe(), cat_cols),
    ]
)

# ---------- 7) åˆ‡é©—è­‰é›† ----------
X_tr, X_va, y_tr, y_va = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ---------- 8) æ¨¡åž‹ 1ï¼šLogistic Regressionï¼ˆbaselineï¼‰ ----------
log_reg = Pipeline(steps=[
    ("prep", preprocess),
    ("model", LogisticRegression(max_iter=1000))
])
log_reg.fit(X_tr, y_tr)
va_pred_lr = log_reg.predict_proba(X_va)[:, 1]
auc_lr = roc_auc_score(y_va, va_pred_lr)
print(f"\n Logistic Regression ROC-AUCï¼š{auc_lr:.4f}")
print("\n LR Validation reportï¼ˆthreshold=0.5ï¼‰")
print(classification_report(y_va, (va_pred_lr >= 0.5).astype(int)))

# retrain å…¨è³‡æ–™ + é æ¸¬ testï¼ˆå…©ä½å°æ•¸ï¼‰
log_reg.fit(X, y)
prob_test_lr = log_reg.predict_proba(X_test)[:, 1]
sub_lr = pd.DataFrame({
    "ID": test_clean["ID"].values,        # â† æ–°çš„é€£è™Ÿ ID
    "Revenue": prob_test_lr.round(2)
})
sub_lr.to_csv(OUT_DIR / "submission_logreg.csv", index=False)
print(f" å·²è¼¸å‡ºï¼š{OUT_DIR/'submission_logreg.csv'}")


# ---------- 9) æ¨¡åž‹ 2ï¼šRandom Forestï¼ˆåŠ åˆ†ï¼‰ ----------
rf = Pipeline(steps=[
    ("prep", preprocess),
    ("model", RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        n_jobs=-1
    ))
])
rf.fit(X_tr, y_tr)
va_pred_rf = rf.predict_proba(X_va)[:, 1]
auc_rf = roc_auc_score(y_va, va_pred_rf)
print(f"\n RandomForest ROC-AUCï¼š{auc_rf:.4f}")
print("\n RF Validation reportï¼ˆthreshold=0.5ï¼‰")
print(classification_report(y_va, (va_pred_rf >= 0.5).astype(int)))

# retrain å…¨è³‡æ–™ + é æ¸¬ testï¼ˆå…©ä½å°æ•¸ï¼‰
rf.fit(X, y)
prob_test_rf = rf.predict_proba(X_test)[:, 1]
sub_rf = pd.DataFrame({"ID": test_ids, "Revenue": prob_test_rf.round(2)})
sub_rf.to_csv(OUT_DIR / "submission_rf.csv", index=False)
print(f" å·²è¼¸å‡ºï¼š{OUT_DIR/'submission_rf.csv'}")

# ---------- 10) å°çµ ----------
print("å…©æ¨¡åž‹æ•ˆèƒ½æ¯”è¼ƒï¼ˆROC-AUC è¶Šé«˜è¶Šå¥½ï¼‰")
print(pd.DataFrame({
    "model": ["LogisticRegression", "RandomForest"],
    "val_roc_auc": [auc_lr, auc_rf]
}).sort_values("val_roc_auc", ascending=False).to_string(index=False))
print("ä»»å‹™å®Œæˆï¼šè«‹åˆ° artifacts/ å¤¾çœ‹å…©å€‹ submission æª”ï¼ˆå·²å››æ¨äº”å…¥åˆ°å°æ•¸é»žå¾Œå…©ä½ï¼‰ï¼Œä»¥åŠå‰/å¾Œé è¦½èˆ‡æ¸…æ´—å¾Œè³‡æ–™ã€‚")


# ===== 11) Feature Importance å¯è¦–åŒ–ï¼ˆéš¨æ©Ÿæ£®æž— + é‚è¼¯å›žæ­¸ï¼‰=====
import numpy as np
import matplotlib.pyplot as plt

def _clean_feat_names(prep, feature_cols):
    """
    å¾ž ColumnTransformer å–å‡º OneHot å±•é–‹å¾Œçš„æ¬„ä½åç¨±ï¼Œä¸¦åŽ»æŽ‰ 'num__' / 'cat__' å‰ç¶´
    """
    try:
        names = prep.get_feature_names_out(feature_cols)
    except TypeError:
        names = prep.get_feature_names_out()
    return [n.split("__", 1)[1] if "__" in n else n for n in names]

def plot_top_importances(pipeline, feature_cols, out_path, top_n=15, title="Top Feature Importance"):
    """
    æ”¯æ´æ¨¹æ¨¡åž‹çš„ feature_importances_ï¼Œæˆ–ç·šæ€§æ¨¡åž‹çš„ coef_ï¼ˆå–çµ•å°å€¼ï¼‰ã€‚
    æœƒåŒæ™‚è¼¸å‡º PNG åœ–èˆ‡ CSV æ˜Žç´°ã€‚
    """
    prep = pipeline.named_steps["prep"]
    model = pipeline.named_steps["model"]

    feat_names = _clean_feat_names(prep, feature_cols)

    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
    elif hasattr(model, "coef_"):
        importances = np.abs(model.coef_.ravel())
        title = title.replace("Feature Importance", "Top Coefficients (|coef|)")
    else:
        raise ValueError("æ­¤æ¨¡åž‹æ²’æœ‰ feature_importances_ æˆ– coef_ å±¬æ€§")

    fi = pd.DataFrame({"feature": feat_names, "importance": importances})
    fi_sorted = fi.sort_values("importance", ascending=False)

    # å­˜æˆ CSVï¼ˆå®Œæ•´è¡¨ï¼‰
    csv_path = out_path.with_suffix(".csv")
    fi_sorted.to_csv(csv_path, index=False)

    # å–å‰ top_n ç¹ªåœ–ï¼ˆæ°´å¹³é•·æ¢ã€ç”±å°åˆ°å¤§ç•«èµ·æ¯”è¼ƒå¥½çœ‹ï¼‰
    top = fi_sorted.head(top_n).iloc[::-1]
    plt.figure(figsize=(9, 6))
    plt.barh(top["feature"], top["importance"])
    plt.title(title)
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()

    print(f" å·²è¼¸å‡ºåœ–æª”ï¼š{out_path}")
    print(f" å·²è¼¸å‡ºæ˜Žç´°ï¼š{csv_path}")

# é‡å°ã€Œå…¨è³‡æ–™é‡è¨“å¾Œã€çš„å…©å€‹æ¨¡åž‹ç”¢å‡ºåœ–
plot_top_importances(
    rf, feature_cols,
    OUT_DIR / "feature_importance_rf.png",
    top_n=15,
    title="Top Feature Importance (RandomForest)"
)

plot_top_importances(
    log_reg, feature_cols,
    OUT_DIR / "feature_importance_lr.png",
    top_n=15,
    title="Top Feature Importance (Logistic Regression)"
)

# --- metrics_from_confusion.py (æ”¾é€²ä½ çš„ main.py ä»»ä¸€æ®µå³å¯åŸ·è¡Œ) ---
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt

# ========= 1) è¼¸å…¥ï¼šå››å€‹æ•¸å­—ï¼ˆTP, FP, FN, TNï¼‰ =========
CONF = {
    "Model A": {"TP": 853, "FP": 341, "FN": 576, "TN": 7230},
    "Model B": {"TP": 846, "FP": 316, "FN": 583, "TN": 7255},
}

# ========= 2) å°å·¥å…·ï¼šç”±æ··æ·†çŸ©é™£è¨ˆç®—å¤šç¨®æŒ‡æ¨™ =========
def calc_metrics(TP, FP, FN, TN):
    total = TP + FP + FN + TN
    def safe_div(a, b): 
        return a / b if b != 0 else 0.0

    accuracy     = safe_div(TP + TN, total)
    precision    = safe_div(TP, TP + FP)
    recall       = safe_div(TP, TP + FN)
    specificity  = safe_div(TN, TN + FP)
    f1           = safe_div(2 * precision * recall, precision + recall)
    fpr          = safe_div(FP, FP + TN)
    fnr          = safe_div(FN, FN + TP)
    balanced_acc = (recall + specificity) / 2

    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1": f1,
        "Specificity": specificity,
        "FPR": fpr,
        "FNR": fnr,
        "BalancedAcc": balanced_acc,
        "TP": TP, "FP": FP, "FN": FN, "TN": TN, "Total": total
    }


# ========= 3) å½™æ•´æˆè¡¨æ ¼ =========
rows = []
for name, c in CONF.items():
    rows.append({"Model": name, **calc_metrics(**c)})
df = pd.DataFrame(rows).set_index("Model")

# å°æ•¸é»žæ ¼å¼å‹å–„è¼¸å‡º
display_cols = ["TP","FP","FN","TN","Total","Accuracy","Precision","Recall","F1","Specificity","FPR","FNR","BalancedAcc"]
print("\n=== Metrics from Confusion Matrix ===")
print(df[display_cols].round(3))

# ========= 4) è¼¸å‡º CSV èˆ‡ åœ–æª” =========
ART = Path("artifacts"); ART.mkdir(exist_ok=True)
df.round(6).to_csv(ART / "metrics_from_confusion.csv")

# 4.1 æŒ‡æ¨™æ¯”è¼ƒé•·æ¢åœ–ï¼ˆAccuracy/Precision/Recall/F1/Specificityï¼‰
plot_cols = ["Accuracy","Precision","Recall","F1","Specificity"]
ax = df[plot_cols].plot(kind="bar", figsize=(9,5))
ax.set_title("Model Metrics Comparison (from Confusion Matrix)")
ax.set_ylabel("Score")
ax.set_ylim(0, 1.0)
for p in ax.patches:
    ax.annotate(f"{p.get_height():.3f}", (p.get_x()+p.get_width()/2, p.get_height()),
                ha='center', va='bottom', fontsize=8, rotation=0, xytext=(0,3), textcoords="offset points")
plt.tight_layout()
plt.savefig(ART / "metrics_bar.png", dpi=150)
plt.close()

# 4.2 å„æ¨¡åž‹æ··æ·†çŸ©é™£ç†±åœ–ï¼ˆä¸ä½¿ç”¨ seabornï¼‰
# 4.2 å„æ¨¡åž‹æ··æ·†çŸ©é™£ç†±åœ–ï¼ˆä¸ä½¿ç”¨ seabornï¼‰
def save_cm_image(TP, FP, FN, TN, title, path):
    import numpy as np
    # è¡Œ=Actual [Positive, Negative]ï¼›åˆ—=Predicted [Positive, Negative]
    cm = np.array([[TP, FN],
                   [FP, TN]])

    fig, ax = plt.subplots(figsize=(4.2, 3.8))
    im = ax.imshow(cm, aspect="auto")
    ax.set_title(title)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    ax.set_xticks([0, 1]); ax.set_xticklabels(["Positive", "Negative"])
    ax.set_yticks([0, 1]); ax.set_yticklabels(["Positive", "Negative"])

    vmax = cm.max()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            val = cm[i, j]
            ax.text(j, i, f"{val}", ha="center", va="center",
                    color="white" if val > vmax/2 else "black")
    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()


save_cm_image(**CONF["Model A"], title="Confusion Matrix â€” Model A", path=ART / "cm_modelA.png")
save_cm_image(**CONF["Model B"], title="Confusion Matrix â€” Model B", path=ART / "cm_modelB.png")

print(f"\nSaved:\n- {ART/'metrics_from_confusion.csv'}\n- {ART/'metrics_bar.png'}\n- {ART/'cm_modelA.png'}\n- {ART/'cm_modelB.png'}")


