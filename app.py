# app.py â€” Streamlit Inference App (CRISP-DM friendly, cleaned-data first)
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support
)

# -----------------------
# Config & Constants
# -----------------------
st.set_page_config(page_title="Hotel Conversion Prediction", layout="wide")
st.title("Hotel Conversion Prediction")

TARGET = "Revenue"
ID_COL = "ID"
# ä¾ä½ å°ˆæ¡ˆæ¬„ä½ï¼šMonth/OS/Browser/Region/TrafficType/VisitorType/Weekend ç‚ºé¡åˆ¥
CANDIDATE_CAT_COLS = ["Month","OperatingSystems","Browser","Region","TrafficType","VisitorType","Weekend"]

# -----------------------
# Utils
# -----------------------
def show_df(df: pd.DataFrame, keep_index: bool = False):
    """çµ±ä¸€è¡¨æ ¼é¢¨æ ¼ï¼šé è¨­éš±è—ç´¢å¼•ï¼ˆé¿å…å·¦é‚Šå‡ºç¾ 0/1/2/3/4 é‚£æ¬„ï¼‰ã€‚"""
    try:
        if keep_index:
            st.dataframe(df, use_container_width=True)
        else:
            st.dataframe(df, use_container_width=True, hide_index=True)
    except TypeError:
        # èˆŠç‰ˆ streamlit ç„¡ hide_index åƒæ•¸ â†’ ç”¨ reset_index é€€è€Œæ±‚å…¶æ¬¡
        st.dataframe(df if keep_index else df.reset_index(drop=True), use_container_width=True)

def make_ohe():
    # sklearn >=1.2 uses sparse_output; older uses sparse
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=True)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=True)

def build_preprocess(num_cols, cat_cols):
    return ColumnTransformer(
        transformers=[
            ("num", StandardScaler(with_mean=False), num_cols),
            ("cat", make_ohe(), cat_cols),
        ]
    )

def coerce_target(y: pd.Series) -> pd.Series:
    if y.dtype == "O":
        return y.astype(str).str.lower().map({"true":1,"yes":1,"1":1,"false":0,"no":0,"0":0}).astype(int)
    return y.astype(int)

def clean_train(df: pd.DataFrame) -> pd.DataFrame:
    # æŒ‰ä½ çš„ç­–ç•¥ï¼štrain ç¼ºå€¼æ¥µå°‘ â†’ ç›´æ¥ä¸Ÿåˆ—
    return df.dropna().copy()

def fill_test(df: pd.DataFrame) -> pd.DataFrame:
    # ä¸åˆªåˆ—ï¼›æ•¸å€¼è£œä¸­ä½æ•¸ã€æ–‡å­—è£œçœ¾æ•¸
    out = df.copy()
    num_cols = out.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if out[c].isnull().any():
            out[c] = out[c].fillna(out[c].median())
    obj_cols = [c for c in out.columns if c not in num_cols]
    for c in obj_cols:
        if out[c].isnull().any():
            mode = out[c].mode(dropna=True)
            out[c] = out[c].fillna(mode.iloc[0] if not mode.empty else "Unknown")
    return out

@st.cache_data(show_spinner=False)
def load_train_csv(path: str = "train.csv"):
    return pd.read_csv(path)

@st.cache_data(show_spinner=False)
def load_train_clean_csv(path: str = "artifacts/train_clean.csv"):
    # è‹¥ä¸å­˜åœ¨æœƒå™´éŒ¯ â†’ å‘¼å«ç«¯ç”¨ try/except
    return pd.read_csv(path)

@st.cache_data(show_spinner=False)
def train_models(train_df: pd.DataFrame):
    """å›å‚³ï¼šfeature_cols, cat_cols, num_cols, models(dict), val_metrics(dict)"""
    assert TARGET in train_df.columns, f"è¨“ç·´è³‡æ–™ç¼ºå°‘ç›®æ¨™æ¬„ä½ {TARGET}"
    assert ID_COL in train_df.columns, f"è¨“ç·´è³‡æ–™ç¼ºå°‘æ¬„ä½ {ID_COL}"

    # å†ªç­‰è™•ç†ï¼šè‹¥å·²æ˜¯ä¹¾æ·¨è³‡æ–™ï¼Œdropna ä¸æœƒæ”¹è®Š
    train_clean = clean_train(train_df)
    y = coerce_target(train_clean[TARGET].copy())
    feature_cols = [c for c in train_clean.columns if c not in [TARGET, ID_COL]]

    # æ¨å°é¡åˆ¥æ¬„ä½ï¼ˆä»¥ä½ çµ¦çš„æ¸…å–®ç‚ºå„ªå…ˆï¼Œfallback ç”¨ dtypeï¼‰
    cat_cols = [c for c in CANDIDATE_CAT_COLS if c in feature_cols]
    num_cols = [c for c in feature_cols if c not in cat_cols]

    preprocess = build_preprocess(num_cols, cat_cols)

    X = train_clean[feature_cols].copy()
    X_tr, X_va, y_tr, y_va = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # å…©å€‹æ¨¡å‹
    lr = Pipeline(steps=[("prep", preprocess), ("model", LogisticRegression(max_iter=1000))])
    rf = Pipeline(steps=[("prep", preprocess), ("model", RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))])

    models, metrics = {}, {}
    for name, pipe in [("LogisticRegression", lr), ("RandomForest", rf)]:
        pipe.fit(X_tr, y_tr)
        va_proba = pipe.predict_proba(X_va)[:, 1]
        auc = roc_auc_score(y_va, va_proba)
        y_hat = (va_proba >= 0.5).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(y_va, y_hat, average="binary", zero_division=0)
        metrics[name] = {
            "roc_auc": float(auc),
            "precision_pos": float(prec), "recall_pos": float(rec), "f1_pos": float(f1),
            "report": classification_report(y_va, y_hat, output_dict=True),
        }
        models[name] = pipe

    # ä»¥å…¨è³‡æ–™é‡è¨“ï¼ˆæ¨è«–ç”¨ï¼‰
    for name in models:
        models[name].fit(X, y)

    return feature_cols, cat_cols, num_cols, models, metrics

def extract_feature_names(prep: ColumnTransformer, feature_cols):
    try:
        names = prep.get_feature_names_out(feature_cols)
    except TypeError:
        names = prep.get_feature_names_out()
    return [n.split("__", 1)[1] if "__" in n else n for n in names]

def plot_feature_importance(pipe: Pipeline, feature_cols, title="Top Feature Importance", top_n=20):
    prep = pipe.named_steps["prep"]
    model = pipe.named_steps["model"]
    feat_names = extract_feature_names(prep, feature_cols)
    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
        label = "Importance"
    elif hasattr(model, "coef_"):
        importances = np.abs(model.coef_.ravel())
        title = title.replace("Feature Importance", "Top Coefficients (|coef|)")
        label = "|coef|"
    else:
        st.info("æ­¤æ¨¡å‹ä¸æ”¯æ´ feature importance / coefficientsã€‚")
        return
    fi = pd.DataFrame({"feature": feat_names, "importance": importances}).sort_values("importance", ascending=False)
    top = fi.head(top_n).iloc[::-1]
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.barh(top["feature"], top["importance"])
    ax.set_title(title)
    ax.set_xlabel(label)
    fig.tight_layout()
    st.pyplot(fig)
    with st.expander("ä¸‹è¼‰å®Œæ•´é‡è¦æ€§æ˜ç´° CSV"):
        csv_bytes = fi.to_csv(index=False).encode("utf-8")
        st.download_button("Download feature_importance.csv", csv_bytes, file_name="feature_importance.csv", mime="text/csv")

def threshold_report(y_true, proba, thr: float):
    y_hat = (proba >= thr).astype(int)
    cm = confusion_matrix(y_true, y_hat, labels=[0,1])
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_hat, average="binary", zero_division=0)
    return cm, prec, rec, f1, classification_report(y_true, y_hat, output_dict=True)

# -----------------------
# Sidebar â€” Controls
# -----------------------
with st.sidebar:
    st.header(" è¨­å®š")
    st.caption("è¨“ç·´è³‡æ–™ï¼šè‡ªå‹•åµæ¸¬ `artifacts/train_clean.csv` â†’ è‹¥ç„¡å‰‡ä½¿ç”¨ `train.csv`ï¼ˆå…§éƒ¨ dropnaï¼‰")
    model_choice = st.selectbox("æ¨¡å‹", ["RandomForest", "LogisticRegression"], index=0)
    thr = st.slider("Decision Threshold (é è¨­ 0.5)", min_value=0.05, max_value=0.95, value=0.50, step=0.01)
    reindex_id = st.checkbox("å°‡ä¸Šå‚³çš„ test.ID è½‰ç‚º 1..Nï¼ˆä¸¦ä¿ç•™ OriginalIDï¼‰", value=False)
    uploaded = st.file_uploader("ä¸Šå‚³ test.csv", type=["csv"])

# -----------------------
# Training Phase (cached)
# -----------------------
# å„ªå…ˆæ¡ç”¨ artifacts/train_clean.csvï¼ˆèˆ‡ main.py çµæœä¸€è‡´ï¼‰ï¼Œå¦å‰‡é€€å› raw + dropna()
try:
    raw_df = load_train_csv("train.csv")
except Exception as e:
    st.error(f"è®€å– train.csv å¤±æ•—ï¼š{e}")
    st.stop()

try:
    clean_df = load_train_clean_csv("artifacts/train_clean.csv")
except Exception:
    clean_df = None

train_df_used = clean_df if clean_df is not None else clean_train(raw_df)
feature_cols, cat_cols, num_cols, models, val_metrics = train_models(train_df_used)

# å·¦ä¸Šè§’æ¦‚è¦½å¡ï¼ˆé¡¯ç¤ºæ¸…æ´—å¾Œåˆ—æ•¸ + deltaï¼‰
raw_rows = len(raw_df)
clean_rows = len(train_df_used)

c1, c2, c3, c4 = st.columns(4)
c1.metric("Train Rows (clean)", f"{clean_rows:,}", delta=f"-{raw_rows - clean_rows:,}")
c2.metric("Train Cols", f"{train_df_used.shape[1]}")
c3.metric("Model AUC (RF)", f"{val_metrics['RandomForest']['roc_auc']:.3f}")
c4.metric("Model AUC (LR)", f"{val_metrics['LogisticRegression']['roc_auc']:.3f}")

st.caption("Train Rows ä¾†æºï¼š" + ("artifacts/train_clean.csv" if clean_df is not None else "ç”± app ä»¥ dropna() æ¸…ç†è‡ª train.csv"))

# é¡¯ç¤ºé©—è­‰æˆç¸¾è¡¨
st.subheader("ğŸ“Š é©—è­‰æˆç¸¾ï¼ˆHold-out 8:2ï¼‰")
score_tbl = pd.DataFrame([
    {"model": "RandomForest", "roc_auc": val_metrics["RandomForest"]["roc_auc"],
     "precision_pos": val_metrics["RandomForest"]["precision_pos"],
     "recall_pos": val_metrics["RandomForest"]["recall_pos"],
     "f1_pos": val_metrics["RandomForest"]["f1_pos"]},
    {"model": "LogisticRegression", "roc_auc": val_metrics["LogisticRegression"]["roc_auc"],
     "precision_pos": val_metrics["LogisticRegression"]["precision_pos"],
     "recall_pos": val_metrics["LogisticRegression"]["recall_pos"],
     "f1_pos": val_metrics["LogisticRegression"]["f1_pos"]},
]).sort_values("roc_auc", ascending=False)
show_df(score_tbl)

# Feature importance / coefficients
st.subheader("ğŸ” ç‰¹å¾µé‡è¦æ€§ / ä¿‚æ•¸")
plot_feature_importance(models[model_choice], feature_cols, title=f"Top Feature Importance ({model_choice})", top_n=20)

# -----------------------
# Inference on Uploaded Test
# -----------------------
st.subheader("ğŸ§ª ä¸Šå‚³æ¸¬è©¦æª”é æ¸¬")
if uploaded is not None:
    try:
        test_df_raw = pd.read_csv(uploaded)
    except Exception as e:
        st.error(f"è®€å–ä¸Šå‚³æª”æ¡ˆå¤±æ•—ï¼š{e}")
        st.stop()

    st.write("**æª”æ¡ˆå‰ 5 åˆ—**")
    show_df(test_df_raw.head())

    # åŸºæœ¬æ¬„ä½æª¢æŸ¥
    missing_cols = [c for c in feature_cols + [ID_COL] if c not in test_df_raw.columns]
    if missing_cols:
        st.error(f"ä¸Šå‚³æª”ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_cols}")
        st.stop()

    # æ¸…ç†èˆ‡ï¼ˆå¯é¸ï¼‰é‡ç·¨ ID
    test_clean = fill_test(test_df_raw)
    if reindex_id:
        test_clean = test_clean.copy()
        test_clean["OriginalID"] = test_clean[ID_COL]
        test_clean[ID_COL] = range(1, len(test_clean) + 1)

    # æ¨è«–
    X_test = test_clean[feature_cols].copy()
    proba = models[model_choice].predict_proba(X_test)[:, 1]
    preds = (proba >= thr).astype(int)

    # é æ¸¬æ‘˜è¦
    st.write("**é æ¸¬æ‘˜è¦**")
    a1, a2, a3 = st.columns(3)
    a1.metric("Rows", f"{len(test_clean):,}")
    a2.metric("Avg Prob.", f"{proba.mean():.3f}")
    a3.metric("Predicted Positives", int(preds.sum()))

    # ä¸‹è¼‰ submission
    sub = pd.DataFrame({ID_COL: test_clean[ID_COL].values, "Revenue": np.round(proba, 2)})
    st.download_button(
        "ğŸ“¥ ä¸‹è¼‰ submission.csv",
        data=sub.to_csv(index=False).encode("utf-8"),
        file_name="submission.csv",
        mime="text/csv",
    )

    # é è¦½ Top N
    st.write("**Top 30 æ©Ÿç‡**ï¼ˆç”±é«˜åˆ°ä½ï¼‰")
    top = test_clean[[ID_COL]].copy()
    top["Probability"] = proba
    top30 = top.sort_values("Probability", ascending=False).head(30)
    show_df(top30)

    # è‹¥ä½¿ç”¨è€…ä¹Ÿæä¾›äº†æ¨™è¨˜ï¼ˆå°‘è¦‹ï¼‰ï¼Œå¯åšè©•ä¼°
    if TARGET in test_clean.columns:
        st.info("åµæ¸¬åˆ°ä¸Šå‚³æª”åŒ…å«ç›®æ¨™æ¬„ä½ `Revenue`ï¼Œä»¥ä¸‹ç‚ºå³æ™‚è©•ä¼°ï¼š")
        y_true = coerce_target(test_clean[TARGET])
        cm, p, r, f1, rep = threshold_report(y_true, proba, thr)
        st.write(f"ROC-AUC: **{roc_auc_score(y_true, proba):.4f}**")
        # æ··æ·†çŸ©é™£èˆ‡å ±å‘Šï¼šä¿ç•™ç´¢å¼•/æ¬„æ¨™ç±¤ï¼ˆé€™è£¡ä¸éš±è—ï¼‰
        cm_df = pd.DataFrame(cm, index=["True 0","True 1"], columns=["Pred 0","Pred 1"])
        show_df(cm_df, keep_index=True)
        rep_df = pd.DataFrame(rep).T
        show_df(rep_df, keep_index=True)
else:
    st.warning("è«‹åœ¨å·¦å´ä¸Šå‚³ `test.csv` ä»¥ç”¢ç”Ÿé æ¸¬èˆ‡ä¸‹è¼‰ submissionã€‚")

st.caption("Â© 2025 Hotel Conversion Prediction â€” CRISP-DM pipeline | Logistic Regression & RandomForest | OneHot + StandardScaler")
